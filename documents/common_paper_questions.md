## FAQ
* #### FAQ
    * ##### Common questions
    * ##### Comparsions of datasets and applications/potentials of our dataset for future research
    * ##### Issues with contribution and what constitutes the released dataset
    * ##### Responses for one reviewer
    * ##### Responses for one reviewer
## Common questions
* #### Contributions and applications of AbdomenAtlas-8K.
    Two contributions: A large-scale dataset of 8,448 annotated CT volumes and an active learning procedure that can quickly create many other large-scale datasets. Firstly, AbdomenAtlas-8K was a composite dataset that unified medical datasets from at least 26 different hospitals worldwide. In total, more than 60.6 x 10<sup>9</sup> voxels were annotated in AbdomenAtlas-8K in comparison with 4.3 x 10<sup>9</sup> voxels annotated in the existing public datasets. We scaled up the organ annotation by a factor of 15. Once released, AbdomenAtlas-8K can be used to benchmark existing segmentation models and foster medical foundation models for a range of downstream applications. Secondly, the proposed active learning procedure can generate an attention map to highlight the regions to be revised by radiologists, reducing the annotation time from 30.8 years to three weeks. This strategy can scale up annotations quickly for creating medical datasets or even natural imaging datasets.
* #### Source and permissions to release data.
    We have now elaborated on the source and permissions in Table 3 (supplementary). To clarify, we will only disseminate the annotations of the CT volumes separately, and users will retrieve the original CT volumes, if needed, from the original sources (websites). Everything we intend to create and license-out will be in separate files and no modifications are necessary to the original CT volumes. We have consulted with the lawyers at Johns Hopkins University, confirming the permissions of distributing the annotations based on the license of each dataset. We will further include detailed download instructions on our GitHub page.
## Comparsions of datasets and applications/potentials of our dataset for future research
* #### Comprehensive description and comparison between AMOS, AbdomenCT-1K, TotalSegmentator and AbdomenAtlas-8K
    1. **A significantly larger number of annotated CT volumes.** TotalSegmentator, AMOS, and AB1K provided 1,204, 500, and 1,112 annotated CT volumes. AbdomenAtlas-8K provided 8,448 annotated CT volumes (around eight times larger).
    2. **A notably greater diversity of the provided CT volumes.** The CT volumes in AbdomenAtlas-8K were collected and assembled from at least 26 different hospitals worldwide, whereas the makeup of TotalSegmentator and AMOS was sourced from a single country. Specifically, TotalSegmentator was from Switzerland (biased to the Central European population) and AMOS was from China (biased to the East Asian population). While AbdomenCT-1K was from 12 different hospitals, our AbdomenAtlas-8K presents significantly more CT volumes (8,448 vs. 1,112) and more types of annotated classes (8 vs. 4). 
    3. **The manual annotation time was significantly reduced.** The creation of AbdomenAtlas-8K used an effective active learning procedure, reducing the annotation time from 30.8 years to three weeks (see the revised Section 3.3 for a detailed calculation). This is an important scientific attempt to put active learning into practical use.
    4. **Produce an attention map to highlight the regions to be revised.** The attention maps mentioned in our active learning procedure can accurately detect regions with a high risk of prediction errors (evidenced in Table 1). This capability enables annotators to quickly find areas that require human revision. As a result, it significantly reduces annotators' workload and annotation time by a factor of 533.

    The  following Table to signify the extension of this research over the previous ones.
    |  dataset name  | # of CT volumes  | # of annotated organs | # of hospitals | use of active learning |
    |  ----  | ----  |  ----  | ----  | ----  |
    | AMOS | 500 | 15 | 2 | No |
    | AbdomenCT-1K | 1,112 | 4 | 12 | No |
    | TotalSegmentator | 1,204 | 104 | 1 | No |
    | AbdomenAtlas-8K | 8,448 | 8 | 26 | Yes |
* #### The utility and scope of the dataset, how it might be employed by future researchers and benchmarks against existing methods to outline future problems to be solved are all vital aspects that are currently missing?
    1. **Benchmarking existing segmentation models.** In the submission, we benchmarked the most recent advances in medical segmentation, i.e., SwinUNETR [[Tang et al., CVPR 2022](https://openaccess.thecvf.com/content/CVPR2022/papers/Tang_Self-Supervised_Pre-Training_of_Swin_Transformers_for_3D_Medical_Image_Analysis_CVPR_2022_paper.pdf)]. Thanks for your suggestion, we have now enriched the comparison by evaluating three more models on the AbdomenAtlas-8K dataset, namely U-Net, UNETR, and SegResNet. The revised benchmark has been added to the main paper (see Table 2). 
    2. **A large dataset for developing medical foundation models.** Developing foundation models for healthcare has recently raised much attention. Foundation model refers to an AI model that is trained on a large dataset and can be adapted to many specific downstream applications. This requires a large-scale, fully-annotated dataset. We anticipate that our AbdomenAtlas-8K can play an important role in achieving this by enabling the model to capture complex organ patterns, variations, and features across different imaging phases, modalities, and a wide range of populations. This has been partially evidenced by our recent publication [[Liu et al., ICCV 2023](https://arxiv.org/abs/2301.00785)] and an ongoing project, showing that fully-supervised pre-training on AbdomenAtlas-8K transfers much better than existing self-supervised pre-training. 
    3. **The proposed strategy can scale up annotations quickly.** This strategy can be used for creating many medical datasets (across organs, diseases, and imaging modalities) or even natural imaging datasets. We are implementing our active learning procedure to re-create the natural imaging datasets previously produced by us [[He et al., ECCV 2022
    ](https://arxiv.org/abs/2112.00933); [Zhao et al., ECCV 2022](https://arxiv.org/abs/2111.14341)], yielding a considerably reduced amount of labeling efforts. Moreover, our strategy is being integrated into open-source software such as MONAI-LABEL at NVIDIA and ChimeraX at UCB/UCSF. This will make a difference in the rapid annotation of medical images in the near future.
    4. **Enabling precision medicine for various downstream applications.**
    We showcased one of the most pressing applications—early detection and localization of pancreatic cancer, an extremely deadly disease, with a 5-year relative survival rate of only 12% in the United States. The AI trained on a large, private dataset at Johns Hopkins Hospital (JHH), performed arguably higher than typical radiologists [[Xia et al., medRxiv 2022](https://www.medrxiv.org/content/10.1101/2022.09.24.22280071v1)]. But this AI model and annotated dataset were inaccessible due to the many policies. Now, our paper demonstrated that using AbdomenAtlas-8K (100% made up of publicly accessible CT volumes), AI can achieve similar performance when directly tested on the JHH dataset (see Table 2). This study is a concrete demonstration of how AbdomenAtlas-8K can be used to train AI models that can be generalized to many CT volumes from novel hospitals and be adapted to address a range of clinical problems.

    [Follow-up Plans] Upon the dataset's release, we aim to host international competitions in 2024, addressing the challenges in multi-organ and multi-tumor segmentation at scale via platforms such as MICCAI/RSNA/Grand Challenge. Additionally, we are in the process of launching special issues about scaling datasets, annotations, and algorithms in leading medical imaging journals such as MEDIA and TMI.
## Issues with Contribution and What Constitutes the Released Dataset
* ### The contribution seems to be the release and the improvement of labels for the public datasets, which have previously been released. It does not appear that this is a novel dataset, rather, a release of improved data labels on existing datasets. It was not immediately clear to me that the proposed labels offered more accurate information over existing labels.
    It is true that we did not introduce new CT volumes. After summarizing the existing public datasets (see Table 3), we found that a combination of these datasets is fairly large (a total of 8,000 CT volumes and many more are coming over the years). The main challenge, however, is the absence of comprehensive annotations. In fact, scaling up annotations is much harder than scaling up CT volumes due to the limited time of expert radiologists. Our contribution is, therefore, significant, covering two major perspectives.
    1. **More comprehensive annotations.** We provide per-pixel annotations for eight organs across over 8000 CT volumes, encompassing various phases and diverse populations. The existing labels of the public datasets are partial and incomplete, e.g., LiTS only had labels for the liver and liver tumors, and KiTS only had labels for the kidneys and kidney tumors. Our AbdomenAtlas-8K offered detailed annotations for all eight organs within each CT volume. As praised by reviewer nhxF: *“As currently the largest dataset, the work significantly contributed to the field of abdominal CT imaging and can serve as the basis of the development of effective AI algorithms in the future.”*
    2. **A fast procedure to annotate CT volumes at scale.** Our active learning procedure enables us to accomplish the task within three weeks, a remarkable contrast to the conventional manual annotation approach, which typically requires about 30.8 years. This accelerates the annotation process by an impressive factor of 533. The majority of the annotation workload is managed solely by only one radiologist. Furthermore, this strategy also allows us to efficiently annotate a wider range of organs and tumors in the future.
    3. [Minor] **Novel dataset.** We have applied our active learning procedure to creating a large private dataset (JHH) of 5,281 novel CT volumes with per-voxel annotations. However, at the moment, we cannot guarantee that this dataset can be released soon due to numerous regulatory constraints. We are actively working with Johns Hopkins Hospital to expedite its potential release. This has been clarified in our introduction: “we commit to releasing 3,410 of the 8,448 CT volumes.”
* ### In any case, the source and permissions to release data needs to be clarified and provided. Information of the data reuse from previous datasets need to be clarified.
    We have now elaborated on the source and permissions in Table 3 (supplementary). To clarify, we will only disseminate the annotations of the CT volumes, whereas the corresponding CT volumes can be downloaded from their original websites. We have consulted with the lawyers at Johns Hopkins University, confirming the permissions of distributing the annotations based on the license of each dataset (summarized below). We will further include this information on our GitHub page and dataset download page, ensuring clarity for users upon the dataset usage.

    |  dataset name  | # of CT volumes  | # of annotated organs | use of active learning | license |
    |  ----  | ----  |  ----  | ----  | ----  |
    | AMOS | 500 | 15 | No | CC BY 4.0 | 
    | AbdomenCT-1K | 1,112 | 4 | No | CC BY 4.0 |
    | TotalSegmentator | 1,204 | 104 | No | CC BY 4.0 |
    | AbdomenAtlas-8K | 8,448 | 8 | Yes | pending |
* ### In addition, since the active learning procedure is one of the key contributions, I expected to see an improvement after its use. However, in **revised** Table 5 (supplementary), the improvement on JHH, the improvement with respect to mDice and mNSD is typically 1-2 points, within the standard deviation for each of the 9 organs and average. Additional information about the utility of the dataset needs to be provided, and whether the provided labels are better than the existing partial labels.
    Table 5 (supplementary) only showed the improvement from Step 0 to Step 1 of the active learning procedure, instead of presenting the full improvement before and after using our active learning procedure. We appreciate your valuable feedback and have made several revisions to this table.

    1. We added the improvement of Step 2 in the active learning procedure. Compared to the results in step 0, both mDSC and mNSD show substantial improvement, confirming the effectiveness of our active learning procedure. 
    2. We marked the revised classes at each Step. The segmentation performance of some organs were marginally improved simply because (1) these organs were hardly revised at certain Steps or (2) the segmentation performance was already very high (e.g., DSC > 90%). For those revised organs (e.g., aorta), AI models showed a significant improvement from 72.3% to 82.9%. 
* ### In Figure 3 (manuscript), the authors report visualizations of the sums of attention maps of each organ. The intention of the experiment and the choice of the 5% threshold was unclear.
    1. **The intention behind Figure 3** is to visualize the distribution of the attention size of each CT volume. A larger attention size implies a greater requirement for revision in various regions.  Figure 3 suggests that the attention size for most CT volumes is small, but there are several significant-sized outliers. These outliers are of high priority for revision by human experts. According to the figure, the ratio of outliers is about 5% (highlighted in red). The 5% is estimated by the plot and also related to the budget of human revision for each Step in the active learning procedure. It is essential to emphasize that roughly 5% of CT volumes within each dataset are highly likely to contain predicted errors, requiring further revision by our annotator. 
    2. **The choice of the 5% threshold.** It is true that this number is empirical. The 5% is estimated based on (1) empirical observations (the number of outliers in Figure 3) and (2) the annotation budget at each Step in the active learning procedure. If there are many outliers or a limited budget, the threshold needs to be increased accordingly. If one is dealing with new datasets, this threshold is easily obtained by analyzing the attention size distribution as plotted in Figure 3.
* ### It seems like the study used 3 human annotators, but the procedure of assignment of annotators was not explained.
    Our study recruited three annotators, comprising a senior radiologist with over 15-year experience and two junior radiologists with 3-year experience. The senior radiologist undertook the task of annotation revision in the active learning procedure. The junior radiologists were responsible for reviewing the completed AbdomenAtlas-8K annotations and making revisions if needed. Additionally, they conducted the inter-annotator variability analysis in Figure 8 and further recorded the time required for annotating each organ in Table 4.
* ### Does the active learning model improve performance of a single annotator, or all annotators. In other works, if the segmentations from all three annotators were combined (without the proposed strategy), would they be better or worse than segmentation acquired using the proposed approach?
    Only one annotator was assigned to revise the annotations generated by AI models. We did not observe a substantial improvement in the annotator's performance in organ segmentation. This is because annotating organ boundaries is relatively less difficult for human experts with 15-year experience than annotating tumors. Therefore, our strategy has rather minor contributions to enhancing the annotation performance of human annotators. For organ segmentation, the pressing challenge lies in the time-consuming nature of the conventional full organ annotation process. In this regard, our strategy accelerates over 500 times compared with conventional ones.
* ### Information about the training level of the annotators is missing.
    One annotator is a trained radiologist with 15 years of experience. The other two additional annotators are with 3 years of experience. Thanks for your question; we have clarified this in the revised manuscript. 
* ### In **revised** Table 4 (supplementary), human annotation time is recorded for each of the three annotators. In the abstract, it is mentioned that the method cuts down annotation time from 30.8 years to three weeks, however, no information is provided about timing for a specific process. It would be important to clarify time improvement of annotating a single scan with and without the proposed method and explain the offline time commitment for training and testing the model.
    1. **Why 30.8 years?** We considered an 8-hour workday, five days a week. A trained annotator typically needs 60 minutes per organ per CT volume [[Park et al., 2020](https://www.sciencedirect.com/science/article/pii/S2211568419301391)]. Our AbdomenAtlas-8K has a total of eight organs and around 8,000 CT volumes. Therefore, annotating the entire dataset requires 60x8x8000 (minutes) / 60/8/5 = 1600 (weeks) = 30.8 (years).
    2. **Why three weeks?** Using our active learning procedure, only 400 CT volumes require manual revision from the human annotator (15 minutes per volume). That is, we managed to accelerate the annotation process by a factor of 60x8/15=32 per CT volume. Therefore, we completed the entire annotation procedure within three weeks as reported in the paper. Human efforts: 400x15 (minutes) / 60/8= 12.5 (days) plus the time commitment for training and testing AI models taking approximately 8.5 (days).
* ### In Table 1, the authors report results comparing attention masks and ground truth with respect to sensitivity and precision. The choice of metrics is not clear. Why not additionally measure specificity and recall, or use metrics such as mDice and mNSD used elsewhere in the submission?
    1. **Why sensitivity and precision**. We chose sensitivity and precision because Table 1 strived to evaluate an error detection task rather than a segmentation task (comparing the boundary of attention maps and error regions). Once an error is detected, it counts as a hit, otherwise, as a miss. Sensitivity and precision can measure how well the attention maps detect the real error regions and whether the errors being detected are real errors, respectively. Hence, these two metrics are appropriate. 
    2. **Why not specificity, recall, mDice, or mNSD?** Recall and sensitivity are the same by [definition](https://en.wikipedia.org/wiki/Sensitivity_and_specificity). Given that the non-error region (true negatives) significantly outweighs the error region, specificity approaches a value close to one (see the table below). Thereby, specificity cannot give us meaningful information on the error detection performance. Moreover, mDice and mNSD are commonly used to evaluate the similarity of two masks, but as discussed above, the similarity in the boundary between attention maps and error regions is not necessary. Following your suggestion, we computed mDice, mNSD, and specificity in the table below. You can see that they do not provide meaningful information on the error detection performance of attention maps.

    |  organ  | mDice(%)  | mNSD(%) | Specificity |
    |  ----  | ----  |  ----  | ----  | 
    | Spleen | 1.0 | 1.5 | 0.99  |
    | Right Kidney | 10.1 | 11.8 | 0.99  |
    | Left Kidney | 12.3 | 15.2 | 0.99 |
    | Gallbladder | 11.7 | 17.5 | 0.99 |
    | Liver | 19.1 | 21.1 | 0.99  |
    | Stomach | 26.3 | 29.3 | 0.99 |
    | Aorta | 27.4 | 20.4 | 0.99 |
    | Postcava (IVC) | 18.8 | 20.8 | 0.99 |
    | Pancreas | 18.5 | 24.0 | 0.99 |
* ### The authors correctly point out that one of the limitations of the approach is evaluating the proposed methodology with respect to disease (e.g., when tumors are present). It would also be important to highlight that the algorithm used in the active learning step is supervised, therefore, any label biases from labels in the training datasets may potentially be propagated into the labeling of the new data.
    Indeed, as discussed in our limitation section, tumor annotation is much more challenging than organ annotation. It is true that when annotators make mistakes on tumor annotations, it will also propagate the AI training and predicting along the supervised active learning procedure. As an extension of this study, we will add tumor annotations to the AbdomenAtlas-8K  dataset by addressing the challenge in two possible directions. **Firstly**, we plan to recruit more experienced radiologists to revise tumor annotations. **Secondly**, we will incorporate the radiology reports (based on biopsy results) into the human revision. These actions can reduce potential label biases and label errors from human annotators.
<!-- Q11: *The data is not released yet. In addition, it is unclear what data will be recycled from previous datasets and how this point relates to the licensing of said previous dataset.*

A11: The dataset will be made publicly available once the paper has been accepted. At the moment, all requisite code for AbdomenAtlas-8K creation has been released on GitHub and we have confirmed with lawyers at Johns Hopkins University for the permissions necessary for distributing the full annotations. To clarify, we will only disseminate the annotations of the CT volumes separately, and users will retrieve the original CT volumes, if needed, from the original sources (websites). Everything we intend to create and license-out will be in separate files and no modifications are necessary to the original CT volumes. -->
* ### It would help if the authors could compare (perhaps create a table) with source countries to better visualize where data from previous datasets and the proposed AbdomenAtlas-8K come from.
    We appreciate your valuable suggestion. In the updated Table 3, we have incorporated the source countries corresponding to each previous dataset. Furthermore, we have included a global map illustrating the worldwide source distribution of AbdomenAtlas-8K.
* ### Whether the annotation procedures across hospitals or annotators may differ (e.g., due to training levels or other factors), and how this may positively or negatively affect performance.
    Consistency and accuracy are of utmost importance in medical data annotation. However, in the real world, variations in annotation procedures can occur across hospitals, laboratories, or individual annotators. When creating AbdomenAtlas-8K, we observed the variations in annotation procedures across 26 hospitals in our study. Two factors could cause these variations based on our observation.
    1. **Different scanning body range.** An illustrative instance is the aorta, an elongated anatomical structure traversing both the thoracic and abdominal cavities. Notably, its morphology exhibits significant differences between these distinct anatomical regions. For abdominal CT scans, in general, the aortic arch in the chest region is typically not scanned. However, due to variations in the scanning range of CT scans across different hospitals, some CT scans also include and annotate the aortic arch. As a result, the standard annotation for the same organ provided by different hospitals varies significantly. 
    2. **Different annotation protocols across hospitals.** For example, the stomach and duodenum often have blurry boundaries, posing a challenge in distinguishing between these two organs. Different hospitals typically adhere to varying annotation protocols, which can further complicate the learning process for our AI model.\\
    **Positive impact to AI training:**
    - Variability to some extent could enhance the robustness of AI models. If a model is trained on a diverse set of annotations from different sources, it might be better equipped to generalize to novel data.\\
    **Negative impact to AI training:**
    - AI models trained on inconsistent annotations could produce unreliable or unpredictable outcomes. As a result, it might be challenging to reproduce AI predictions across different hospitals if there's significant variability in the source annotations.



